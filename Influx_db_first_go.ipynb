{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a walk through of what I will do to download and install InfluxDB and other components of the InfluxDB echo system. The reason for doing this is out of general couriousity. I have worked with timeseries data in the past and want to see if InfluxDB really makes this easier to work with.\n",
    "\n",
    "## Disclaimers and such\n",
    "\n",
    "I don't have a lot of experiance with InfluxDB. My only exposure has been job ad's on linked in and reading through some of the docs they have made available over the course of an evening. I may (... meaning I probably will) do things that are anti-influxdb-ish. I appologize in advance and appreachiate any feedback. Finally, any opinions I have are my own and do not necessarily reflect those of any organizations I work for or have worked for or any organizations that I am otherwise affiliated with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install InfluxDB\n",
    "\n",
    "I am going to start with installing InfluxDB locally. There are also Docker Images which I may expose at some point. The overarching goal is to get data into InfluxDB. There are various data creators and sample data sets that I can use - which I will be sure to document. Unless they are propritary I will also provide code or point to the applicable repos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl -sL https://repos.influxdata.com/influxdb.key | sudo apt-key add -\n",
    "source /etc/lsb-release\n",
    "echo \"deb https://repos.influxdata.com/${DISTRIB_ID} ${DISTRIB_CODENAME} stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\n",
    "```\n",
    "\n",
    "The instructions found here are not working: https://docs.influxdata.com/influxdb/v1.4/introduction/installation/\n",
    "\n",
    "That being said, I found a seperate set of instructions here: https://portal.influxdata.com/downloads#influxdb\n",
    "\n",
    "```\n",
    "wget https://dl.influxdata.com/influxdb/releases/influxdb_1.4.2_amd64.deb\n",
    "sudo dpkg -i influxdb_1.4.2_amd64.deb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = subprocess.Popen(['sudo', 'systemctl', 'start', 'influxdb'], stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['influxdb  6719  0.0  2.2 303504 22716 ?        Ssl  16:51   0:02 /usr/bin/influxd -config /etc/influxdb/influxdb.conf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idb_running():\n",
    "    proc = subprocess.Popen(['ps', 'aux'], stdout=subprocess.PIPE)\n",
    "    return [p for p in proc.stdout.read().split('\\n') if 'influx' in p]\n",
    "idb_running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = subprocess.Popen(['sudo', 'systemctl', 'stop', 'influxdb'], stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idb_running()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get some test data in the DB\n",
    "Let's face it what I just did is not all that interesting... and we probably have a bit to go before I do something that is interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_idb(silent=False):\n",
    "    out = subprocess.Popen(['sudo', 'systemctl', 'start', 'influxdb'], stdout=subprocess.PIPE)\n",
    "    if not silent:\n",
    "        sleep(1)\n",
    "        pprint(idb_running())\n",
    "    \n",
    "def stop_idb(silent=False):\n",
    "    out = subprocess.Popen(['sudo', 'systemctl', 'stop', 'influxdb'], stdout=subprocess.PIPE)\n",
    "    if not silent:\n",
    "        sleep(1)\n",
    "        pprint(idb_running())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['influxdb 28561 34.0  2.1 156040 21608 ?        Ssl  17:44   0:00 /usr/bin/influxd -config /etc/influxdb/influxdb.conf']\n"
     ]
    }
   ],
   "source": [
    "start_idb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I might come to forget this but it appears that there are a couple of key data types:\n",
    "\n",
    "* timestamp\n",
    "* fieldkeys and fieldvalues\n",
    "* tag keys and tag values\n",
    "\n",
    "In a manufacturing context I tend to use these terms a lot, although the way they are used are slightly seperate. For example I would typically define a tag as a combination of:\n",
    "\n",
    "* timestamp\n",
    "* tag name\n",
    "* tag value\n",
    "\n",
    "which would typically look like this:\n",
    "\n",
    "`[ datetime(2017, 03, 03, 01, 22, 55, ...), 'temp_ava1', 8877 ] `\n",
    "\n",
    "Anyways - I will try to remain consistant with influxdb's termonology. I think what makes the most sense to do is create a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also looks like there is a library for python: https://github.com/influxdata/influxdb-python\n",
    "\n",
    "`pip install influxdb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb import InfluxDBClient\n",
    "import influxdb\n",
    "import json\n",
    "import copy\n",
    "import numbers\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBNAME = 'first_try'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InfluxDBClient(database=DBNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0.0\n",
      "[{u'name': u'_internal'}]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print influxdb.__version__\n",
    "print client.get_list_database()\n",
    "print client.get_list_measurements()\n",
    "# print client.get_list_privileges()\n",
    "# print client.get_list_retention_policies()\n",
    "print client.get_list_users()\n",
    "# print client.get_list_series()  # should have been looking at release not master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_database(DBNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'name': u'_internal'}, {u'name': u'first_try'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_list_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Asset(object):\n",
    "    \"\"\"\n",
    "    I am modeling this after IIoT. An asset is a machine or test station that produces data. Ideally I am providing metadata\n",
    "    (tag keys and values) that will describe the asset.\n",
    "    \n",
    "    It should be noted that I have not looked at Telegraf too much, so I may be replicating it's functionality\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, asset_name, fixed=True, refrence_id=None):\n",
    "        \"\"\"\n",
    "        :param asset_name: name of the asset\n",
    "        :param fixed: If the asset is fixed in place - True. If it is a mobile asset (e.g. car, truck, trailer, etc) - False\n",
    "        :param refrence_id: assets tend to be part of systems. Other systems are pretty good at tracking structure.... this is a \n",
    "                            tie out\n",
    "        \"\"\"\n",
    "        \n",
    "        self.asset_name = asset_name\n",
    "        self.fixed = fixed\n",
    "        self.ref_id = refrence_id\n",
    "        self.template = self._create_boiler_plate()\n",
    "    \n",
    "    def _create_boiler_plate(self):\n",
    "        bp = {\n",
    "                \"measurement\": None,\n",
    "                \"tags\": {\n",
    "                    \"asset_name\": self.asset_name,\n",
    "                    \"fixed\": self.fixed,\n",
    "                    \"ref_id\": self.ref_id\n",
    "                },\n",
    "                \"time\": None,\n",
    "                \"fields\": {\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return bp\n",
    "    \n",
    "    def update_static_tags(self, tags):\n",
    "        if not isinstance(tags, dict):\n",
    "            raise TypeError('There is a strong assumption that tags is a dictionary')\n",
    "        self.template['tags'].update(tags)\n",
    "        \n",
    "class Measurement(object):\n",
    "    \"\"\"\n",
    "    Create Measurements that will then be written with values added. There will also be a chance to add Tags that are variable \n",
    "    (for example, something like a serial number or part number)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m_name, asset):\n",
    "        self.m_name = m_name\n",
    "        self.rec = copy.deepcopy(asset.template)\n",
    "        self.rec['measurement'] = m_name\n",
    "        self.timezone = 'UTC'\n",
    "        self.dlst = False\n",
    "        \n",
    "    def set_timezone(self, timezone, dlst=False):\n",
    "        if timezone in pytz.all_timezones:\n",
    "            self.timezone = timezone\n",
    "        else:\n",
    "            raise ValueError('{} if not a valid timezone. Try import pytz; pytz.all_timezones; for a valid list')\n",
    "        self.dlst = dlst\n",
    "        \n",
    "    def update_fields(self, fields, timestamp=None):\n",
    "        \n",
    "        # validate strong assumptions\n",
    "        bad_keys = [k for k in fields if not isinstance(k, basestring)]\n",
    "        if len(bad_keys) > 0:\n",
    "            raise ValueError('Keys in fields must be a base string. Bad values include: {}'.format(bad_keys))\n",
    "        bad_values = [v for v in fields.values() if not isinstance(v, numbers.Real)]\n",
    "        if len(bad_values) > 0:\n",
    "            raise ValueError('Values in fields must be an int or float. Bad values include: {}'.format(bad_values))\n",
    "            \n",
    "        self.rec['fields'].update(fields)\n",
    "        \n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now()\n",
    "        \n",
    "        local_dt = pytz.timezone(self.timezone).localize(timestamp, is_dst=self.dlst)\n",
    "        utc_dt = local_dt.astimezone(pytz.utc)\n",
    "        \n",
    "        self.rec['time'] = utc_dt\n",
    "        \n",
    "    def update_tags(self, tags):\n",
    "        # validate strong assumptions\n",
    "        bad_keys = [k for k in tags if not isinstance(k, basestring)]\n",
    "        if len(bad_keys) > 0:\n",
    "            raise ValueError('Keys in tags must be a base string. Bad values include: {}'.format(bad_keys))\n",
    "        bad_values = [v for v in tags.values() if not isinstance(v, basestring)]\n",
    "        if len(bad_values) > 0:\n",
    "            raise ValueError('Values in fields must be a base string. Bad values include: {}'.format(bad_values))\n",
    "            \n",
    "        self.rec['tags'].update(tags)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_machine = Asset('magic_machine', refrence_id='mm44004')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_rate = Measurement('magic_rate', magic_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_rate.update_fields({'magic_flow': 45.12023, 'magic_flow_set': 47.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_rate.update_tags({'station_no': '3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "points.append(magic_rate.rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.write_points(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.query('select * from machine_rate;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.query('select * from magic_rate;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResultSet({'(u'magic_rate', None)': [{u'station_no': u'3', u'fixed': u'True', u'ref_id': u'mm44004', u'asset_name': u'magic_machine', u'time': u'2017-12-24T21:58:06.481515776Z', u'magic_flow_set': 47, u'magic_flow': 45.12023}]})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall impressions\n",
    "\n",
    "Overall this has been pretty much a hello world version so it is hard to say what I think. I have a feeling that the real power of this database is going to be in the performance from a write persepective and other components of the architecture. Over the next couple of days I probably take a look at those. I am trying to figure out why I would not just use something like Elastic Search for this activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
